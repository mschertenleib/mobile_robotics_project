{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d95cf84d-7ffc-43a2-b921-67a4bf1271d8",
   "metadata": {},
   "source": [
    "# <center/> MICRO-452 - Basics of Mobile Robotics \n",
    "### <center/> Thymio Robot Project\n",
    "#### <center/> TEAM 12\n",
    "\n",
    "***\n",
    "__<p style='text-align: left;'> Group members & Sciper</p>__ \n",
    "<br/> Jacques Bénand 325957\n",
    "<br/> Mathieu Schertenleib 313318\n",
    "<br/> Théo Maetz 312583\n",
    "<br/> Maria Cherchouri 311251\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c59bb5-231a-40bf-9808-1f6425f0c716",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff0407a-6825-4d82-8109-0fb62b9b5012",
   "metadata": {},
   "source": [
    "This Jupyter notebook provides all the information you need about the choices we've made for this project and the theory behind our implementation, to run the project directly, simply execute the main() cell before the difficulties encountered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701aa4d3-3628-4355-90ec-5dd1520dee9f",
   "metadata": {},
   "source": [
    "Remember that the aim was to control our Thymio robot by executing 4 separate components: vision, global navigation, local navigation, and filtering which we'll assemble later.  \n",
    "After this reassembly, the Thymio must move from an arbitrary position in the map to a target that can be placed anywhere in the environment while avoiding the global obstacles set by the map. During its navigation, it must also be able to avoid local obstacles that may be placed on its path. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5dcdfe-7804-448c-a0a8-52c766f29243",
   "metadata": {},
   "source": [
    "We use ArUco markers to detect the four corners of the map, as well as the robot and the target. A visibility graph is used to implement the path from the robot to the goal while avoiding obstacles, Astolfi controller for control and Kalman filter for filtering. Finally, we'll use our robot's proximal sensors to avoid 3D local obstacles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b19835-14dc-4053-94c1-63c2b82d59f2",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "The environment is a white background made by taping two A0 sheets of paper length-wise. This allows for a clear contrast from the fixed obstacle in black placed on top of the map, cut up from black sheets of paper as to obtain a desired shape. These obstacles are wilfully flat as to not trigger the proximity sensors of the Thymio, which are only to be triggered in the case of a surprise 3 dimensional obstacle and activate the local avoidance protocol. A camera is setup above the map in such a way that all 4 markers defining the map are visible in the frame. This  will function with any set up from the camera as long as these 4 markers are visible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8c753a-71c5-4ae5-9ed7-ba3b5b9e01be",
   "metadata": {},
   "source": [
    "### Code structure\n",
    "\n",
    "The folder src/ contains all the files used to run the project. Namely:\n",
    "- **main.py** runs the whole system, manages the interface, the main control loop and calls the different submodules\n",
    "- **parameters.py** stores the global constants needed throughout the different submodules\n",
    "- **calibrate_camera.py** implements a simple interface for the user to calibrate the camera\n",
    "- **threaded_capture.py** implements a video capture on its own thread\n",
    "- **image_processing.py** implements all functions related to computer vision and coordinate transformations\n",
    "- **global_map.py** manages the building and updating of the obstacle mask, the global map and pathfinding\n",
    "- **kalman_filter.py** implements the Kalman filter algorithm\n",
    "- **controller.py** implements our (slightly modified) Astolfi controller\n",
    "- **local_navigation.py** implements local obstacle detection and avoidance\n",
    "\n",
    "The folder tests/ contains various tests done during the development of this project. Most of them can be run independently, and they might be unit tests (for example for segment intersection), benchmarks or higher level programs, like the one included in this notebook for interactive map visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6409587-98c5-477a-94b9-599cdc5a2b43",
   "metadata": {},
   "source": [
    "### Libraries used\n",
    "\n",
    "- [tdmclient](https://pypi.org/project/tdmclient/) for connecting to the robot\n",
    "- [OpenCV](https://opencv.org/) for image processing and computer vision\n",
    "- [NumPy](https://numpy.org/) for general array processing and linear algebra\n",
    "- [DearPyGui](https://github.com/hoffstadt/DearPyGui) for the user interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba35038-0c0f-4e1b-83e4-486776e04492",
   "metadata": {},
   "source": [
    "### Overall implementation and structure\n",
    "\n",
    "The flow chart of the overall global implementation of the project is provided in the figure below\n",
    "\n",
    "![](images/flow_chart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d072bbca-9354-4739-9eb6-114ea663f82d",
   "metadata": {},
   "source": [
    "### I. Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8d4c51",
   "metadata": {},
   "source": [
    "* ##### **ArUco Markers**\n",
    "     \n",
    "     \n",
    "     To determine the position and orientation of the Thymio, it is of the utmost importance to determine the corresponding point in our real-life environment to a point in our 2D image projection. This task is facilitated by the use of ArUco markers generated directly from the OpenCV library. Their respectively unique black and white patterns are optimized as to make their detection as fast as possible whilst easily differentiating them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2952cb3d",
   "metadata": {},
   "source": [
    "* ##### **Camera Calibration using ArUco ChessBoard**\n",
    "     \n",
    "     \n",
    "     When calibrating the camera, an ArUco chessboard ChArUco is generated similar to this one:\n",
    "    <div>\n",
    "    <img src=\"images\\charucoboard.png\" width=\"300\"/>\n",
    "    </div>\n",
    "\n",
    "    [Image of a ChArUco board](https://docs.opencv.org/3.4/df/d4a/tutorial_charuco_detection.html)\n",
    "\n",
    "    To calibrate using a ChArUco board, it is necessary to detect the board from different viewpoints.\n",
    "    The ChArUco corners and ChArUco identifiers captured on each viewpoint are then stored.\n",
    "    The calibrateCamera() function will fill the camera_matrix and distortion_coeffs arrays with the camera calibration parameters, which is then stored in a json file. \n",
    "\n",
    "    Now, we can take an image and undistort it. However first, we can refine the camera matrix using cv2.getOptimalNewCameraMatrix(), and then show the undistorted frame. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603dcba2",
   "metadata": {},
   "source": [
    "* ##### **Marker Detection**\n",
    "\n",
    "     \n",
    "     \n",
    "     - Principle:\n",
    "    \n",
    "        The principle of the marker detection used by OpenCV is explained in detail, [here](https://docs.opencv.org/4.x/d5/dae/tutorial_aruco_detection.html).\n",
    "\n",
    "        When using the `detectMarkers()`method of the ArucoDetector class we use the predefined default detector parameters. Here we want to explain what these parameters are to better understand how the markers are so efficiently detected.\n",
    "\n",
    "        The first parameters concern the adaptive thresholding of the frame, to determine how many thresholding steps are performed onto the frame as well as the minimum and maximum adaptive Threshold Window sizes (see OpenCV [adaptiveThreshold()](https://docs.opencv.org/4.x/d7/d1b/group__imgproc__misc.html#ga72b913f352e4a1b1b397736707afcde3) function)\n",
    "\n",
    "        After finishing the thresholding step, the contours of the markers are determined. At this stage contours for non marker candidates are filtered out in multiple steps. Afterwards, the bit of each candidate is analysed to determine whether or not they are actually a marker and of course if they are a marker of the marker dictionary defined beforehand.  \n",
    "        \n",
    "        \n",
    "     \n",
    "    - Usage:\n",
    "    \n",
    "        We first define a dictionary of our 6 markers with cv2.aruco.extendDictionary() function\n",
    "        The tuples `marker_corners` and `marker_ids` are then assumed to be directly obtained from cv2.aruco.ArucoDetector.detectMarkers(), at each sampling time.\n",
    "\n",
    "        `marker_corners` is a tuple of arrays defining every 4 corner position for each ArUco marker, in their original order (which is clockwise starting from the top left).\n",
    "\n",
    "        `marker_ids` is the list of ids of each marker, to characterize each one.\n",
    "\n",
    "        The markers with IDs 0 through 3 are defined as the map corners in clockwise order, starting from the top left. The markers 4 and 5 are respectively attributed to the robot and the target.\n",
    "\n",
    "        Taking advantage of the ordered corners and their given position, for each marker, we can compute the position & orientation of the Thymio using the function `detect_robot()` and the position of the targetwith `detect_target()`.  It must be noted that a marker can be found rotated in the environment, however, the detection process needs to be able to determine its original rotation, so that each corner is differentiated and identified.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f641fe",
   "metadata": {},
   "source": [
    "* ##### **Image correction**\n",
    "     \n",
    "     \n",
    "     In order to build a map from the camera frame, we need to perform two transformations:\n",
    "     - First of all, we need to correct the lens distortions, by applying a cv2.undistort on the camera frame. The calibration parameters (namely the camera matrix and the distortion coefficients) are read from a json file, which is created by running **src/calibrate_camera.py**.\n",
    "    - Secondly, we need to correct the perspective of the undistorted frame. By using the four detected corners of the map, and the corresponding four corners of the map image, we can compute a perspective transformation matrix using cv2.getPerspectiveTransform. cv2.warpPerspective then allows us to get a perspective correct frame, where each corner is at a marker corner.\n",
    "\n",
    "    This undistorted, perspective corrected frame can be directly used by the map generation algorithm to extract the obstacles, and to detect the position of the robot and the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd99e672",
   "metadata": {},
   "source": [
    "### II. Global navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed6223c",
   "metadata": {},
   "source": [
    "* #### **Parameters**\n",
    "\n",
    "These are the global constants representing the different dimensions of the robot and its environment.\n",
    "\n",
    "| Name                  | Value                          |\n",
    "|-----------------------|--------------------------------|\n",
    "| FRAME_WIDTH           | 960                            |\n",
    "| FRAME_HEIGHT          | 720                            |\n",
    "| MAP_WIDTH_MM          | 1334                           |\n",
    "| MAP_HEIGHT_MM         | 968                            |\n",
    "| MAP_WIDTH_PX          | 900                            |\n",
    "| MAP_HEIGHT_PX         | Computed from map aspect ratio |\n",
    "| SAMPLING_TIME         | 0.1                            |\n",
    "| MMS_PER_MOTOR_SPEED   | 0.4348                         |\n",
    "| TARGET_RADIUS_MM      | 70.0                           |\n",
    "| MARKER_SIZE_MM        | 57.0                           |\n",
    "| ROBOT_RADIUS_MM       | 80                             |\n",
    "| ROBOT_CENTER_TO_WHEEL | 48                             |\n",
    "| ROBOT_WHEEL_RADIUS    | 22                             |\n",
    "| ROBOT_WHEEL_SPACING   | 96                             |\n",
    "| DILATION_RADIUS_MM    | ROBOT_RADIUS_MM + 25           |\n",
    "| ROBOT_MASK_RADIUS_MM  | ROBOT_RADIUS_MM + 20           |\n",
    "| TARGET_MASK_RADIUS_MM | TARGET_RADIUS_MM + 10          |\n",
    "| MARKER_MASK_SIZE_MM   | MARKER_SIZE_MM + 5             |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a269448",
   "metadata": {},
   "source": [
    "* #### **Obstacle mask extraction**\n",
    "     \n",
    "     \n",
    "     The extraction of the obstacles from the perspective corrected frame is done in five main steps:  \n",
    "\n",
    "    - First of all, the grayscale frame is thresholded by a fixed value (of 120). We use an inverse binary threshold, such that all areas that were darker than the threshold become '1' (represents obstacles), and all areas that were lighter than the threshold become '0' (represents free space).\n",
    "    - Secondly, the robot, target and corner markers of the map are masked out from the obstacle mask, as to not be considered as obstacles.\n",
    "    - Then, we use a cv2.MORPH_OPEN morphology operation with a small kernel (5x5), in order to discard tiny obstacle pixel groups (in the range of a few millimeters). This could be dirt on the map, folds in the paper making a sharp shadow, etc. Note that cv2.MORPH_OPEN is just erosion followed by dilation.\n",
    "    - We then add an artificial 1 pixel wide border around the map, to keep the robot fully within the bounds.\n",
    "    - Finally, we dilate the obstacle mask by the robot radius converted to pixels, plus a 25mm margin to allow for uncertainties and errors.\n",
    "\n",
    "    The function **get_obstacle_mask** in **image_processing.py** implements this algorithm.\n",
    "    \n",
    "    We show below all the steps of obstacle extraction, from camera frame to final free space mask.\n",
    "\n",
    "    ![](images/camera_frame.png)\n",
    "\n",
    "  *Frame from the camera (already undistorted in this capture)*\n",
    "\n",
    "    ![](images/obstacle_extraction/warped.png)\n",
    "\n",
    "  *Perspective corrected frame*\n",
    "\n",
    "    ![](images/obstacle_extraction/thresholded.png)\n",
    "\n",
    "  *First raw obstacle mask: thresholded image*\n",
    "    \n",
    "    ![](images/obstacle_extraction/masked.png)\n",
    "\n",
    "  *Masked robot, target and corner markers*\n",
    "\n",
    "    ![](images/obstacle_extraction/opened.png)\n",
    "\n",
    "  *After the MORPH_OPEN operation*\n",
    "\n",
    "    ![](images/obstacle_extraction/borders.png)\n",
    "\n",
    "  *Adding borders*\n",
    "\n",
    "    ![](images/obstacle_extraction/dilated.png)\n",
    "\n",
    "  *Dilation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3db9d48",
   "metadata": {},
   "source": [
    "* #### **Obstacle contours extraction**\n",
    "     \n",
    "     \n",
    "     When we have a mask of the obstacles, we can move on to finding their contours.  \n",
    "     The first step here is to invert the mask, such that a '1' represents free space. The outlines of these zones of free space are extracted using the cv2.findContours function of OpenCV. Note that we only use cv2.CHAIN_APPROX_SIMPLE as an option, which means series of points lying on horizontal, vertical or diagonal lines will be combined. The number of vertices we get at the end is still in the thousands though, and untractable for a visibility graph.  \n",
    "     \n",
    "     The solution is to approximate the contours by a polygon of lower vertex count, using cv2.approxPolyDP. We found that an epilon of 2 pixels gave good results, lowering the total number of vertices to a few hundred at max, while keeping a good approximation of sharp features in the obstacle mask.  \n",
    "     \n",
    "     The findContours function also gives their topological hierarchy, which allows us to group them by \"regions\", which are disconnected regions of the map (i.e. there is no possible path from one to the other). In the subsequent steps where we extract the visibility graph, we can perform checks only within a region, which increases performance. Note that this is hard to demonstrate with the Thymio due to the available size of the map and the need to slit it in two (at least), but it would extend very well to a broader mapped space.  \n",
    "     \n",
    "     The function extract_contours from **global_map.py** implements the steps explained above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78076903",
   "metadata": {},
   "source": [
    "* #### **Visibility graph creation**\n",
    "     \n",
    "     \n",
    "     The visibility graph is created in two steps:\n",
    "\n",
    "    - First of all, the static part of the graph if built (i.e. only edges between obstacle vertices). This is the slowest operation, taking approximately 1 second for the kind of map we tested.\n",
    "    - Secondly, the dynamic part of the graph is built (i.e. the edges between the robot or the target and the obstacle vertices).\n",
    "\n",
    "    About the implementation of the algorithm, two important considerations allow us to get much better performance than a naive approach:\n",
    "    \n",
    "    - Only convex vertices need to be considered. Given any two random vertices on the map, the shortest path between them will never pass by a concave vertex. Geometrically, if we think about a string going around a spiky shape, it will always only touch convex vertices.\n",
    "    - Assume we are considering a given convex vertex, trying to decide with which other ones to consider an edge or not. If the considered vertex has its two neighbors on the contour lying on each \"side\" of the edge, we do not need to include him. Geometrically, any path that would pass by this vertex would always be shorter by going directly to one of its neighbors instead.\n",
    "\n",
    "    When the set of remaining edges, we must make sure they are free of any intersection with all others (which is `O(N^2)`), hence the importance of the pre-filtering.\n",
    "\n",
    "    Also note that the source and target vertices are first projected out of obstacles if they happen to be inside one. This means there will always be a path even if the robot is within the obstacle mask (the red zone). This increases robustness, as the path following controller will always provide an input even if the robot is slightly in the obstacle zone. This is required in the case of a visibility graph, because we are following the borders of obstacle zones, which means we might be within that zone a significant portion of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3b5273",
   "metadata": {},
   "source": [
    "* #### **Pathfinding**\n",
    "     \n",
    "     \n",
    "     When we have the full visibility graph, we use a simple Dijkstra's algorithm to find the shortest path between the source (the robot) and the target. Since the total number of edges in the graph is very low (can be less than 100 for most environments), there is no point in implementing a greedier search such as A*.\n",
    "\n",
    "    ![](images/global_map.png)\n",
    "    *Global map creation and pathfinding on a test image. This is a capture of the intereactive visualization below.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b868ae08",
   "metadata": {},
   "source": [
    "* #### **Interactive map**\n",
    "     \n",
    "     \n",
    "     The full implementation of the map creation from obstacle mask to pathfinding can be interactively tested by running the code below. You can click anywhere to place the source point, and the target is the point under the mouse cursor. This visualization allows to see the vertices of the contours (black being the first one, and white the last one), as well as the visibility graph and the shortest path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed62f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './src')\n",
    "\n",
    "from global_map import *\n",
    "from image_processing import *\n",
    "\n",
    "g_target = (120, 730)\n",
    "g_source = (200, 100)\n",
    "\n",
    "\n",
    "def draw_contour_orientations(img: np.ndarray, contours: list[np.ndarray]):\n",
    "    \"\"\"\n",
    "    Draw positive orientation as green, negative as red;\n",
    "    first vertex is black, last is white\n",
    "    \"\"\"\n",
    "    for c in range(len(contours)):\n",
    "        orientation = np.sign(cv2.contourArea(contours[c], oriented=True))\n",
    "        color = (64, 192, 64) if orientation >= 0 else (64, 64, 192)\n",
    "        cv2.drawContours(img, [contours[c]], contourIdx=-1, color=color, thickness=2)\n",
    "        n_points = len(contours[c])\n",
    "        for i in range(n_points):\n",
    "            brightness = i / (n_points - 1) * 255\n",
    "            cv2.circle(img, contours[c][i], color=(brightness, brightness, brightness), radius=5, thickness=-1)\n",
    "\n",
    "\n",
    "def interactive_map_and_path():\n",
    "    color_image = cv2.imread('images/map_divided.png')\n",
    "    obstacle_mask = get_obstacle_mask(color_image, robot_position=None, target_position=None, mask_corner_markers=False)\n",
    "\n",
    "    approx_poly_epsilon = 2\n",
    "    regions = extract_contours(obstacle_mask, approx_poly_epsilon)\n",
    "\n",
    "    all_contours = [contour for region in regions for contour in region]\n",
    "\n",
    "    free_space = np.empty_like(color_image)\n",
    "    free_space[:] = (64, 64, 192)\n",
    "    cv2.drawContours(free_space, all_contours, contourIdx=-1, color=(255, 255, 255), thickness=-1)\n",
    "\n",
    "    graph = build_graph(regions)\n",
    "\n",
    "    cv2.namedWindow('main', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('main', color_image.shape[1], color_image.shape[0])\n",
    "\n",
    "    while True:\n",
    "        free_source, free_target = update_graph(graph, regions, np.array(g_source), np.array(g_target))\n",
    "        path = dijkstra(graph.adjacency, Graph.SOURCE, Graph.TARGET)\n",
    "\n",
    "        img = cv2.addWeighted(color_image, 0.75, free_space, 0.25, 0.0)\n",
    "        draw_contour_orientations(img, all_contours)\n",
    "        cv2.drawContours(img, all_contours, contourIdx=-1, color=(64, 64, 192))\n",
    "        draw_graph(img, graph)\n",
    "        draw_path(img, graph, path, np.array(g_source), free_source, np.array(g_target), free_target)\n",
    "\n",
    "        cv2.namedWindow('main', cv2.WINDOW_NORMAL)\n",
    "        cv2.setMouseCallback('main', mouse_callback)\n",
    "        cv2.imshow('main', img)\n",
    "        if cv2.waitKey(1) == 27:\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def mouse_callback(event, x, y, flags, param):\n",
    "    global g_target, g_source\n",
    "    if event == cv2.EVENT_MOUSEMOVE:\n",
    "        g_target = (x, y)\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        g_source = (x, y)\n",
    "\n",
    "\n",
    "interactive_map_and_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084c6078",
   "metadata": {},
   "source": [
    "### III. Extended Kalman Filter\n",
    "\n",
    "![](gif/camera_hidden.gif)\n",
    "\n",
    "Kalman Filters are very useful in the case of location estimation. The Kalman filter uses a system model along with measurements and system inputs, to maintain an estimate on our position and orientation.\n",
    "Our filter is based on an odometric model using the motor speed readings as inputs, and the geometry of the robot movement. The Kalman filter relies on the measurements of the position & orientation provided by the camera, to correct the estimate. The state variables of interest here will be the coordinates $x$, $y$ and $\\theta$ defined as in the figure:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f4b4d3",
   "metadata": {},
   "source": [
    "\n",
    "<div>\n",
    "<img src=\"images\\Thymio_schema.jpeg\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "Schematic of the Thymio reference used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44222b0f",
   "metadata": {},
   "source": [
    "We therefore have the state vector $\\textbf{x} = (x,y,\\theta)$.\n",
    "Readings are obtained at a sampling time of $T_s = 0.1 \\, \\text{s}$ and the coordinates are updated by:\n",
    "$$\\begin{bmatrix}\n",
    "    x_{k+1} \\\\ \n",
    "    y_{k+1} \\\\ \n",
    "    \\theta_{k+1}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    x_{k} \\\\ \n",
    "    y_{k} \\\\ \n",
    "    \\theta_{k}\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "    \\frac{\\omega_{r,k}+\\omega_{l,k}}{2}  R_w T_s \\sin\\left({\\theta_k+\\frac{1}{2}\\frac{\\omega_{r,k}-\\omega_{l,k}}{d} R_w T_s}\\right) \\\\ \n",
    "    \\frac{\\omega_{r,k}+\\omega_{l,k}}{2}  R_w T_s \\cos\\left({\\theta_k+\\frac{1}{2}\\frac{\\omega_{r,k}-\\omega_{l,k}}{d} R_w T_s}\\right) \\\\ \n",
    "    2 \\cdot \\frac{1}{2}\\frac{\\omega_{r,k}-\\omega_{l,k}}{d} R_w T_s\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Here $R_w$ is the wheel Radius and d is the space between the wheels.\n",
    "\n",
    "We can further simplify these equations by defining:\n",
    "\n",
    "`tangential_speed`: $\\delta V_k = \\frac{\\omega_{r,k}+\\omega_{l,k}}{2}  R_w T_s$\n",
    "\n",
    "`delta.angle`: $\\delta \\theta_k = \\frac{\\omega_{r,k}-\\omega_{l,k}}{d} R_w T_s$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee6c66",
   "metadata": {},
   "source": [
    "In our case the sate is observed by the camera measurements of $x$, $y$ & $\\theta$ with an independant Gaussian noise process $v$.\n",
    "Therefore our mobile robot can be defined by:\n",
    "$$x_k = g(x_{k-1},u_k) + w_k $$\n",
    "$$y_k = h(x_{k-1}) + v_k $$\n",
    "Here, the function $g$ defines our motion model with $ w_k \\sim N(0, Q)$ is a multivariate normal distribution that models the uncertainty introduced by the state transition. The function $h$ defines our measurement model $v_k \\sim N(0, R)$ is a multinormal describing the measurement noise. Both $w_k$ and $v_k$ are considere as zero-mean white Gaussian Noise, with $Q$ and $R$ as, respectively, the constant covariance of the process noise and the measurements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e07b0e",
   "metadata": {},
   "source": [
    "Denoting our nonlinear function for the motion model $g$, we used an Extended Kalman Filter. In the case of well defined transition models, the EKF has been considered the de facto standard in the theory of nonlinear state estimation, navigation systems and GPS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eb77b3",
   "metadata": {},
   "source": [
    "* #### **Prediction step**\n",
    "     \n",
    "     \n",
    "     The prediction step consists of computing the mean and covariance of the predicted state using linearization of the motion model $g(x_{k-1},u_k)$.  \n",
    "     \n",
    "    The a priori estimated mean of our state is defined by:\n",
    "    $$\\bar{\\mu}_k = g(x_{k-1},u_k)$$  \n",
    "    \n",
    "    Furthermore, the a priori estimated covariance of the state is:\n",
    "    $$\\bar{\\Sigma}_k = G_k \\Sigma_{k-1} G_k^T + Q$$  \n",
    "    \n",
    "    Here $G$ is defined as the Jacobian of the motion model $g$:\n",
    "    $$  G_k = \\left.\\frac{\\partial g}{\\partial \\textbf{x}_k}\\right\\vert_{\\textbf{x}_k = \\bar{\\mu}_k, w_k = \\textbf{0}} =  \\begin{bmatrix}\n",
    "    1 & 0 & \\delta V \\cos(\\theta_k + \\delta \\theta_k)\\\\\n",
    "    0 & 1 & -\\delta V \\sin(\\theta_k + \\delta \\theta_k) \\\\\n",
    "    0 & 0 & 1 \\\\\n",
    "    \\end{bmatrix}$$\n",
    "\n",
    "    We also need to define the constant covariance matrix $Q$ of the process noise. We assumed the standard deviation of on the state position variables $x$ and $y$ to be approximately of 1 millimeter and for the angle $\\theta$ to be of $0.04 \\, \\text{radians}$.   \n",
    "    Therefore obtaining:\n",
    "    \n",
    "    $$Q = \\begin{bmatrix}\n",
    "    1 & 0 & 0 \\\\\n",
    "    0 & 1 & 0 \\\\\n",
    "    0 & 0 & 0.04 \\\\\n",
    "    \\end{bmatrix}$$  \n",
    "    \n",
    "    This covariance matrix was mostly determined through trial and error, tuning the parameters along to obtain the desired result. We did experiment with a relatively high initial covariance and kept the final result at which it converges to, keeping only the diagonal values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0699755e",
   "metadata": {},
   "source": [
    "* #### **Measurement Update**\n",
    "     \n",
    "     \n",
    "     The next step is updating our estimation from the previous step, using the measurements to compute the a posteriori mean and covariance of our estimated state. The goal is to improve the accuracy of the estimated state by incorporating the real-world measurements. We first need to determine the difference between the camera measurements and our a priori estimate, this is the innovation $i$:\n",
    "    $$i = y_k - \\bar{\\mu}_k$$  \n",
    "    \n",
    "    From this, we can determine the measurement prediction covariance $S$ by adding our Measurement covariance matrix $R$:\n",
    "    $$S_k = \\bar{\\Sigma}_k + R$$  \n",
    "    \n",
    "    Which allows us to compute the Kalmain Gain $K$ which specifies the degree to which the measurement is incorporated into the new state estimate. It is computed so to minimize the a posteriori error covariance:\n",
    "    $$K_k =  \\bar{\\Sigma}_k S_k^{-1}$$  \n",
    "    \n",
    "    Finally we obtain our new a posteriori estimate with mean $\\mu_k$ and covariance $\\Sigma_k$:\n",
    "    $$\\mu_k = \\bar{\\mu}_k + K_k i$$\n",
    "    $$\\Sigma_k = \\bar{\\Sigma}_k - K_k \\bar{\\Sigma}_k $$  \n",
    "    \n",
    "\n",
    "    We also need to define our constant measurement covariance R, we determined to be:\n",
    "    $$R = \\begin{bmatrix}\n",
    "    1 & 0 & 0 \\\\\n",
    "    0 & 1 & 0 \\\\\n",
    "    0 & 0 & 0.04 \\\\\n",
    "    \\end{bmatrix}$$  \n",
    "    \n",
    "\n",
    "    The $R$ matrix was more or less defined in the same manner as the $Q$ matrix through tuning the parameters until we got a satisfying result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9207641",
   "metadata": {},
   "source": [
    "* #### **Filtering without the camera measurements**\n",
    "     \n",
    "     \n",
    "     When updating our estimation using the measurements, we need to first consider whether or not these measurements are available. If the camera were to be disturbed or if one of the ArUco markers were obstructed from view, our estilation can no longer rely on these camera measurements.\n",
    "\n",
    "    We implemented this potential scenario by setting $y_k$ as the a priori estimate $\\bar{\\mu}_k$. This makes the innovation null and therefore cancelling out the effects of the Kalman gain on the mean of our a posteriori estimate whilst still adjusting the covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb1f31b",
   "metadata": {},
   "source": [
    "* #### **Implementation**\n",
    "     \n",
    "     Inputs\n",
    "    - `measurements`: (Optional) np.ndarray containing the position $(x,y, \\theta)$  of the robot\n",
    "    - `mu_km`: Previous state estimation mean at time step $(k-1)$\n",
    "    - `sig_km`: Previous state estimation covariance at time step $(k-1)$\n",
    "    - `speed_left`: Left motor speed input of the Thymio\n",
    "    - `speed_right`: Right motor speed input of the Thymio\n",
    "      \n",
    "    Outputs\n",
    "    - `x_est`: New state estimation mean at time step $k$\n",
    "    - `sig_est`: New state estimation covariance at time step $k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c28c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_filter(measurements: Optional[np.ndarray], mu_km: np.ndarray, sig_km: np.ndarray, speed_left: float,\n",
    "                  speed_right: float) -> tuple[np.ndarray, np.ndarray]:\n",
    "    # Since our discretized motion model does not perfectly match reality, try to correct it by some extent.\n",
    "    # Since forward Euler integration tends to extrapolate \"too far\" for smooth trajectories, artificially reduce the\n",
    "    # contribution of the derivative to the state update. Note that this is just a \"hack\" that seems to work well enough\n",
    "    # for this system, and the specific multiplication factor was found through experimentation. For a more precise\n",
    "    # motion model, we should use a better integration scheme (Runge-Kutta 4 for example), and move the Kalman filter\n",
    "    # computation to the hardware, to get significant lower latencies on the inputs, as well as using a higher sampling\n",
    "    # frequency.\n",
    "    EFFECTIVE_SAMPLING_TIME = SAMPLING_TIME * 0.8\n",
    "\n",
    "    tangential_speed = (speed_left + speed_right) / 2\n",
    "    angular_speed = (speed_right - speed_left) / ROBOT_WHEEL_SPACING\n",
    "    delta_angle = angular_speed * EFFECTIVE_SAMPLING_TIME\n",
    "    # Prediction through the a priori estimate\n",
    "    x_est = mu_km[0]\n",
    "    y_est = mu_km[1]\n",
    "    angle_est = mu_km[2]\n",
    "    # Estimated mean of the state\n",
    "    mu_k_pred = np.zeros(3)\n",
    "    mu_k_pred[0] = x_est + tangential_speed * EFFECTIVE_SAMPLING_TIME * -np.sin(angle_est + delta_angle)\n",
    "    mu_k_pred[1] = y_est + tangential_speed * EFFECTIVE_SAMPLING_TIME * np.cos(angle_est + delta_angle)\n",
    "    mu_k_pred[2] = angle_est + delta_angle\n",
    "\n",
    "    # Jacobian of the motion model\n",
    "    G_k = np.eye(3)\n",
    "    G_k[0, 2] = -tangential_speed * np.cos(angle_est + delta_angle)\n",
    "    G_k[1, 2] = -tangential_speed * np.sin(angle_est + delta_angle)\n",
    "\n",
    "    # Estimated covariance of the state\n",
    "    sig_k_pred = G_k @ sig_km @ G_k.T\n",
    "    sig_k_pred += KALMAN_Q\n",
    "\n",
    "    if measurements is not None:\n",
    "        y = measurements\n",
    "    else:\n",
    "        # If no measurements we consider our measurements to be the same as our a priori estimate as to cancel out the\n",
    "        # effect of innovation\n",
    "        y = mu_k_pred\n",
    "\n",
    "    # Innovation / measurement residual\n",
    "    i = y - mu_k_pred\n",
    "\n",
    "    # Correctly handle the case where the angle difference is discontinuous\n",
    "    if i[2] < -np.pi:\n",
    "        i[2] = 2 * np.pi + i[2]\n",
    "    elif i[2] > np.pi:\n",
    "        i[2] = - 2 * np.pi + i[2]\n",
    "\n",
    "    # Measurement prediction covariance\n",
    "    S = sig_k_pred + KALMAN_R\n",
    "\n",
    "    # Kalman gain (tells how much the predictions should be corrected based on the measurements)\n",
    "    K = sig_k_pred @ np.linalg.inv(S)\n",
    "\n",
    "    # A posteriori estimate\n",
    "    x_est = mu_k_pred + K @ i\n",
    "    sig_est = sig_k_pred - K @ sig_k_pred\n",
    "\n",
    "    # Keep the angle within [-pi, pi]\n",
    "    if x_est[2] < -np.pi:\n",
    "        x_est[2] = 2 * np.pi + x_est[2]\n",
    "    elif x_est[2] > np.pi:\n",
    "        x_est[2] = - 2 * np.pi + x_est[2]\n",
    "\n",
    "    return x_est, sig_est"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd84f5bf",
   "metadata": {},
   "source": [
    "### IV. Motion Control\n",
    "\n",
    "![](gif/follow_path.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929ff9de",
   "metadata": {},
   "source": [
    "* #### **Implementation method**  \n",
    "    For the control of the movement of the robot, we use the **Astolfi** controller seen in the course. Using this controller, we get a smooth trajectory betweeen the different points of the path to the goal.  \n",
    "\n",
    "    - **Inputs :**\n",
    "        - *state* : corrected (or predicted if the camera is hidden) state given by the Kalman filter\n",
    "        - *goal_state* : position of the next goal on the global path  \n",
    "        \n",
    "    - **Outputs :**\n",
    "        - *u_r* : speed of the right wheel of the robot (in mm/s)\n",
    "        - *u_l* : speed of the left wheel of the robot (in mm/s)\n",
    "        - Boolean switch : **TRUE** if the goal is attained, **FALSE** if not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba8faa1",
   "metadata": {},
   "source": [
    "* #### **Differences with the course**  \n",
    "     \n",
    "     \n",
    "     The following image represents the configuration for which the Astolfi controller is defined in the course :  \n",
    "     \n",
    "     ![astolfi_course](images\\astolfi_course.png)\n",
    "     \n",
    "     For our implementation, we defined the orientation of the robot differently : instead of having an orientation of 0 degrees when looking in the direction *X*, we decided that the robot had an orientation of 0 degrees when looking in the direction *Y*.  \n",
    "     As seen on the image below :\n",
    "     ![robot_orientation](images\\robot_orientation.png)  \n",
    "     \n",
    "     Hence, the *X* axis defined the course becomes our *Y* axis, and the *Y* axis defined in the course becomes our *-X* axis. This has an impact on the way that the reference angle is calculated (absolute angle made by the normal (our axis *Y*) leaving from the position of the robot and the segment between the position of the robot and the position of the goal).  \n",
    "     ![arctan](images\\arctan.png)  \n",
    "     \n",
    "     As the *Y* axis of the course becomes our *-X* axis, we can compute the reference angle as :  \n",
    "     $ \\theta_R = arctg(\\dfrac{-\\Delta x}{\\Delta y}) $\n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ed53e1",
   "metadata": {},
   "source": [
    "* #### **Code implementation**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92578829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parameters import *\n",
    "\n",
    "\n",
    "def astolfi_control(state, goal_state) -> tuple[float, float, bool]:\n",
    "    \"\"\"\n",
    "    Returns the left desired speed, right desired speed and whether the goal was reached\n",
    "    \"\"\"\n",
    "\n",
    "    Kp = 8  # > 0\n",
    "    Ka = 75  # > Kp\n",
    "    goal_radius = 20\n",
    "\n",
    "    delta_x = goal_state[0] - state[0]\n",
    "    delta_y = goal_state[1] - state[1]\n",
    "    reference_angle = -np.arctan2(delta_x, delta_y)\n",
    "\n",
    "    rho = np.sqrt(delta_x ** 2 + delta_y ** 2)\n",
    "    alpha = reference_angle - state[2]\n",
    "    if alpha < -np.pi:\n",
    "        alpha = 2 * np.pi + alpha\n",
    "    elif alpha > np.pi:\n",
    "        alpha = - 2 * np.pi + alpha\n",
    "\n",
    "    if rho <= goal_radius:\n",
    "        return 0, 0, True\n",
    "\n",
    "    power = 1/4\n",
    "    K_power = 140 / (goal_radius**power)\n",
    "    v = Kp * K_power * rho**power\n",
    "    # v = Kp * rho\n",
    "    omega = Ka * alpha\n",
    "\n",
    "    u_r = (ROBOT_CENTER_TO_WHEEL * omega + v) / ROBOT_WHEEL_RADIUS\n",
    "    u_l = (v - ROBOT_CENTER_TO_WHEEL * omega) / ROBOT_WHEEL_RADIUS\n",
    "\n",
    "    speed_threshold = 80\n",
    "    u_r = np.clip(u_r, -speed_threshold, speed_threshold)\n",
    "    u_l = np.clip(u_l, -speed_threshold, speed_threshold)\n",
    "\n",
    "    return u_l, u_r, False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101a12be",
   "metadata": {},
   "source": [
    "* #### **Gain tuning**  \n",
    "     \n",
    "     \n",
    "     We have the following speeds with the corresponding gains for the Astolfi controller defined in the course :  \n",
    "     ![astolfi_speeds](images\\astolfi_speeds.png)  \n",
    "     We don't want to force the angle **$\\beta$** to go to zero (we don't want the robot to be vertical each time it reaches a goal) so we set the value of the **Kb** (in the picture above $k_\\beta$) parameter really close to zero.  \n",
    "     \n",
    "     The gain controlling the orientation and the angular error of the robot **Ka** (in the picture above $k_\\alpha$) was tuned after testing multiple values and choosing the one that we thought had the best effect.  \n",
    "     \n",
    "     For the linear speed of the robot $v$, we modified a little bit the Astolfi controller from the course. Instead of having the linear speed of the robot being linearly proportional to the distance to the goal, $v$ is now proportional to the fourth root of the distance to the goal. In doing this, the linear speed only drops to a really small value when the robot is really close to the goal instead of dropping linearly when getting close to the goal. This behavior can be seen in the figure below.  \n",
    "     ![speed_diff](images\\speed_diff.png)  \n",
    "     As seen in the picture above, when we get to close to the goal radius (which we defined as 20mm), the speed using our modified controller is way higher than the speed of the controller given in the course. This allows for smoother transitions between goals and an overall smoother trajectory.\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a13672d-7cdc-4152-a64b-8d5692693cf0",
   "metadata": {},
   "source": [
    "* #### **Kidnapping**\n",
    "\n",
    "    Kidnapping works very simply: as soon as the robot or the target moves too much from one frame to the next, we recompute the path.\n",
    "\n",
    "  ![](gif/kidnapping.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff7d261",
   "metadata": {},
   "source": [
    "### V. Local navigation\n",
    "\n",
    "![](gif/local_avoidance.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec33523e",
   "metadata": {},
   "source": [
    "Efficient local navigation is an essential element in the field of autonomous robotics, particularly for mobile robots operating in partially known environments. This strategic approach combines global navigation methods to find a viable path leading to the goal with local navigation mechanisms to avoid obstacles in real time.\n",
    "\n",
    "Let's take the example of an autonomous car: it can't just go straight to the goal without taking into account what's happening in its environment, such as other cars on the road or pedestrians crossing the road.  \n",
    "It has to be able to deal with these local obstacles.  \n",
    "\n",
    "The same applies to our Thymio: it needs to be able to switch from global to local navigation in the event of new local obstacles being detected.  \n",
    "In our case, the switch from local to global is made via our counter **num_samples_since_last_obstacle**.\n",
    "\n",
    "If we're in the global case, our robot will take the speed values returned by our controller, but if we're in our local navigation, the robot's behavior becomes independent of global navigation until our counter reaches a value greater than 3, so until a certain time when we haven't seen an obstacle, at which point the controller takes back control of our robot.\n",
    "\n",
    "We decided to use this attribute to give the robot time to move forward after avoiding an obstacle, before switching back to global navigation to avoid the undesirable effect of our controller. Indeed, it will tend to return to the path and hit the obstacle if it is between the robot and the path, before detecting it due to the radical movement of the controller's commands.\n",
    "\n",
    "See implementation details below or directly in the **main.py** file, in the function **run_navigation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665fa6d7",
   "metadata": {},
   "source": [
    "* #### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193cc2be",
   "metadata": {},
   "source": [
    "|   Name   |   Initial value  |  Description |  \n",
    "|---    |:-:    |:-:        |\n",
    "|   motor_speed   |   200   |   Reference speed of the robot for avoidance | \n",
    "|   threshold   |   17  |   Sensor threshold for obstacle detection | \n",
    "|   prox_gain   |      0.07 |  Gain by which we multiply the sensor values to define the speed at which we turn to avoid the obstacle  | \n",
    "|   num_samples_since_last_obstacle   |   -1    | Counter to switch from local to global navigation |  \n",
    "|   case   |   None   |Integer varying if the obstacle is to the left, to the right, in front of the robot, or even if there is no obstacle |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a4a21",
   "metadata": {},
   "source": [
    "* #### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c79fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Speed command given by our controller\n",
    "speed_left_target = np.clip(int(input_left / MMS_PER_MOTOR_SPEED), -500, 500)\n",
    "speed_right_target = np.clip(int(input_right / MMS_PER_MOTOR_SPEED), -500, 500)\n",
    "\n",
    "# Do local obstacle detection\n",
    "prox_horizontal = [0] * 5\n",
    "for i in range(5):\n",
    "    prox_horizontal[i] = list(nav.node.v.prox.horizontal)[i]\n",
    "    \n",
    "# The \"avoid obstacle\" function will modify and return the counter value; if it detects an obstacle, \n",
    "#it will reset the counter to zero, transitioning to local navigation accordingly.\n",
    "nav.num_samples_since_last_obstacle = aw(\n",
    "    avoid_obstacles(nav.node, nav.num_samples_since_last_obstacle, prox_horizontal))\n",
    "\n",
    "# If we recently saw an obstacle (i.e. more recently than 3 SAMPLING_TIME ago), do not yet switch to path\n",
    "# following, as to hopefully let the robot clear the obstacle.\n",
    "if 0 <= nav.num_samples_since_last_obstacle <= 3:\n",
    "    nav.num_samples_since_last_obstacle += 1\n",
    "elif nav.num_samples_since_last_obstacle > 3:\n",
    "    nav.num_samples_since_last_obstacle = -1\n",
    "\n",
    "# If no obstacle was recently detected, send the controller's desired speeds to the motors\n",
    "elif nav.num_samples_since_last_obstacle == -1:\n",
    "    nav.node.send_set_variables(set_motor_speed(speed_left_target, speed_right_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695250b8",
   "metadata": {},
   "source": [
    "* #### **Obstacle detection**  \n",
    "     \n",
    "     \n",
    "     Ideally, the robot should have access to its local environment at all times, so that it can anticipate obstacle avoidance, especially if the obstacle is moving, and react to it at all times.    \n",
    "     In discrete time, this proves complicated, so we need to set a sampling time for which we retrieve the data. Here, our sampling time is 0.1s.    \n",
    "     We therefore have access to the sensor values every 0.1s, which enables us to react according to the read values.  \n",
    "       \n",
    "     In our program, obstacle detection is performed using our thymio's 5 proximity sensors.  \n",
    "     These sensors use the infrared emission/reception principle, they measure the distance towards an obstacle by generating an infrared signal, and the particular electronics of the horizontal sensor filters the ambient light.  \n",
    "       \n",
    "     We set a threshold defining the detection of an obstacle, and compare this threshold with the values of our sensors. Once this threshold is exceeded, we consider that an obstacle has been detected, and then consider different cases.  \n",
    "     The values of the proximal sensors range from 0 (the robot doesn't detect anything) to several thousand (the robot is very close to an obstacle), knowing that we decide to set the threshold to 17.  \n",
    "       \n",
    "     See details below or directly in the **local_navigation.py** file in the **avoid_obstacle** function.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79981161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If an obstacle is detected, we switch to obstacle avoidance mode instead of goal tracking mode.\n",
    "if ((obst[0] > HTobst or obst[1] > HTobst) and (obst[0] > obst[2] or obst[1] > obst[2])):\n",
    "    # Different cases are distinguished\n",
    "    # Case 0 where the obstacle is on the left\n",
    "    num_samples_since_last_obstacle = 0\n",
    "    case = 0\n",
    "\n",
    "elif ((obst[4] > HTobst or obst[3] > HTobst) and (obst[4] > obst[2] or obst[3] > obst[2])):\n",
    "    # Case 1 where the obstacle is on the right\n",
    "    num_samples_since_last_obstacle = 0\n",
    "    case = 1\n",
    "\n",
    "elif (obst[2] > HTobst and (obst[0] < obst[2] or obst[4] < obst[2]) and (obst[1] <= obst[2] or obst[1] <= obst[2])):\n",
    "    # Case 2 where the obstacle is in front\n",
    "    num_samples_since_last_obstacle = 0\n",
    "    case = 2\n",
    "else:\n",
    "    #In this case we didn't detect obstacles\n",
    "    case = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7541d95",
   "metadata": {},
   "source": [
    "* #### **Obstacle avoidance**  \n",
    "     \n",
    "     \n",
    "     Once the robot has detected potential obstacles, it can avoid them depending on the specific case.  \n",
    "       \n",
    "     In the case where the obstacle has been detected on the sides, the robot will avoid it by turning in the opposite direction of the obstacle.  \n",
    "     In the case where the obstacle is in front of the robot, it will move backwards and turn in a random direction to avoid the obstacle.  \n",
    "     And in each case, the robot will move straight until the counter reaches a value greater than 3 or detects another obstacle.  \n",
    "       \n",
    "     See details below or directly in the **local_navigation.py** file in the **avoid_obstacle** function.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80641474",
   "metadata": {},
   "outputs": [],
   "source": [
    "if case == 0 or case == 1:\n",
    "    # Obstacle is to your left or right.\n",
    "    # Obstacle avoidance: accelerate the wheel near the obstacle to turn in the opposite direction to the obstacle.\n",
    "    if case == 0:\n",
    "        base_speed = prox_gain * (prox[0] + prox[1])\n",
    "        await move_forward(node, int(base_speed), int(base_speed * 0.1))\n",
    "\n",
    "    elif case == 1:\n",
    "        base_speed = prox_gain * (prox[4] + prox[3])\n",
    "        await move_forward(node, int(base_speed * 0.1), int(base_speed))\n",
    "\n",
    "elif case == 2:\n",
    "    # Obstacle is in front of the Thymio.\n",
    "    # Avoid the obstacle in front by backing up and then turning randomly on one side.\n",
    "    if np.random.randint(2) == 0:\n",
    "        await step_back(node, motor_speed)\n",
    "        await turn_left(node, motor_speed)\n",
    "    else:\n",
    "        await step_back(node, motor_speed)\n",
    "        await turn_right(node, motor_speed)\n",
    "\n",
    "elif num_samples_since_last_obstacle >= 0:\n",
    "    await move_forward(node, motor_speed, motor_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b1c0d1",
   "metadata": {},
   "source": [
    "* #### **Example of implementation**  \n",
    "     \n",
    "     \n",
    "     ![speed_diff](gif\\Local_avoidance.gif)  \n",
    "     In this example, we can see that our robot detects our obstacle, avoids it by turning in the opposite direction, keeps going straight ahead until our counter **num_samples_since_last_obstacle** reaches a value of 3, then the controller and global navigation take over, guiding our robot to the goal.    \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01769b7",
   "metadata": {},
   "source": [
    "### VI. Interface\n",
    "\n",
    "![](gif/follow_path.gif)\n",
    "\n",
    "We developped a simple but expressive interface to be able to visualize the whole system while it is running. The three windows include:\n",
    "- The frame from the camera, with the detected map corners drawn as red crosses.\n",
    "- The perspective corrected frame with the obstacle mask, the map graph and the path. We also draw on this image the pose of the robot as seen by the camera (in red), and as estimated by the Kalman filter (in green). The detected target position is also drawn in green.\n",
    "- Plots of the estimated and measured X and Y position of the robot, and a plot of the estimated and measured angle of the robot.\n",
    "\n",
    "On the map window, the button \"Build graph\" (re-)builds the obstacle mask and visibility graph. As soon as the graph changes or the position of the robot or the target varies abnormally, the path is re-computed. The button \"GO!\" starts the path following.\n",
    "\n",
    "On the plots window, the number of plotted samples can be changed with a slider, and the axes auto-fit can be disabled if necessary.\n",
    "\n",
    "The interface was coded using [DearPyGui](https://github.com/hoffstadt/DearPyGui), a very powerful and simple to use graphical interface library based on [Dear ImGui](https://github.com/ocornut/imgui)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3613ddeb",
   "metadata": {},
   "source": [
    "### VII. Final program\n",
    "\n",
    "The final full program can be executed by simply running src/main.py, or alternatively by running the code cell below.\n",
    "\n",
    "Important notice for the user:\n",
    "- The program expects to find a camera.json file in the same directory from which it is run. This file is included in the src/ folder alongside the rest of the code, and corresponds to the last calibration of our camera. It is created by running the file calibrate_camera.py, as explained in the section Camera calibration.\n",
    "- The present version of the code opens the video capture with the DirectShow backend, which was required to be able to change the camera resolution on the test PC. This would obviously need to be changed if run for example on a Mac. There sadly seems to be no VideoCapture setting that allows it to be used everywhere without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bfa0f8-8836-48c2-ad75-05ad15c62a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './src')\n",
    "\n",
    "from main import main\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c7be57",
   "metadata": {},
   "source": [
    "### **VII. Difficulties encountered**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca93b62",
   "metadata": {},
   "source": [
    "* #### **Angle error discontinuity**\n",
    "  \n",
    "  When calculating the angle error between the orientation of the robot and the reference orientation defined by the goal, we use the **numpy** angular function *arctan2* which returns a value in the range [$-\\pi$, $\\pi$]. In our Kalman filter and motion control implementations, this led to wrong behaviors when the error would go just past $-\\pi$ or $\\pi$ as the error value would suddenly jump from a big negative value to a big positive value (or the other way around).  \n",
    "  \n",
    "  We corrected this with the following piece of code that we added to our Kalman filter and motion control implementations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf488df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correctly handle the case where the angle difference is discontinuous\n",
    "\n",
    "if i[2] < -np.pi:\n",
    "    i[2] = 2 * np.pi + i[2]\n",
    "elif i[2] > np.pi:\n",
    "    i[2] = - 2 * np.pi + i[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c22d22b",
   "metadata": {},
   "source": [
    "* #### **Change of controller** \n",
    "     \n",
    "     \n",
    "  We first settled on the use of a PID controller for the implementation of the motion control of our robot. With this controller, the robot would turn towards the goal, once its orientation is equal to the angle defined by the goal plus or minus some angular threshold, it would start to make its way towards the goal. Then once it is in a certain region around the goal, it would switch its objective to the next goal and reiterate the steps. This implementation, although simple at first sight, presented quite a lot of issues for us :  \n",
    "  \n",
    "     - If we set the angular threshold too low, the robot would sometimes never attain the desired orientation and just oscillate around the reference angle.\n",
    "     - If we set the angular threshold too high, the robot would deviate too much from its path to the goal and thus never attain it\n",
    "      - When set on the right path, our robot would not go straight when given same inputs for the speeds of the right and left wheels, which made it deviate from its desired path\n",
    "        \n",
    "    In the end, we decided to abandon this solution and implement the Astolfi controller which worked a lot better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72ae41b",
   "metadata": {},
   "source": [
    "* #### **Local navigation implementation with the Astolfi controller**\n",
    "     \n",
    "     \n",
    "  When first implementing our local navigation process, we had issues to make it work as the Astolfi controller that we used would immediately rectify the robots direction to go back to its original path and thus hit the obstacle.  \n",
    "  \n",
    "  To make this work, we made the robot first avoid the obstacle then go straight for some time before going back into the motion control process. This way, the robot has enough room to go past the obstacle and then go back to its original path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa16c41",
   "metadata": {},
   "source": [
    "* #### **Computer vision with colors points**     \n",
    "        \n",
    "     \n",
    " \n",
    "   At first, we tried to define the environment using points of different color (for example green for the corners of the map, red for the thymio and blue for the goal). Although this worked with good lighting conditions, we quickly found out that it was really dependent of the lighting and thus of where we placed the environment (this worked differently when in the Polydome or when in the CO building). Because of this, we had to tune some parameters or recalibrate the computer vision each time we changed locations or the lighting changed.  \n",
    "     \n",
    "     To resolve this issue, we decided to use **ArUco** markers to define the corners of the map as well as the positions of the Thymio and the goal. This works way better than color points and is most importantly way more robust when it comes to lighting conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6f71fd",
   "metadata": {},
   "source": [
    "* #### **Be careful with the units**  \n",
    "     \n",
    " \n",
    "     \n",
    "  We had quite a number of issues when putting all the systems together because of the use of different units for the different modules of the project. For example, the motion control used millimeters while the Kalman filter used meters at first. Or the computer vision used radians while the motion control used degrees for the angles. This was an easy fix but that still made us lose a bit of time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
